---
title: "Lectures on ML Systems"
author:
    name: Pankaj Pansari
    affiliation: Plaksha University
    url: https://pankajpansari.github.io/about.html
---

This page serves as a reference for understanding Machine Learning Systems (ML Systems). The material consists of video lectures, slides, and additional reading material. 

### Description

Machine learning models are pervasively used to solve problems in varied fields such as vision, robotics, NLP, and scientific discovery. The increased capabilities of these models has corresponded with increase in their size and compute requirements. Besides, use of these models in real-world applications demands strict requirements on performance parameters, such as latency, throughput, and hardware usage efficiency. 

The focus of this course is on exploring these systems-related challenges during training and serving of large language models (LLMs) with special emphasis on Transformer architecture. Topics include GPU architecture and hardware-aware algorithms, ML frameworks and compilers, techniques to parallelize LLMs over multiple GPUs, and reduction of computational complexity and memory footprint.

**Textbook**: The following book can be helpful for parts of the course:

[How to Scale Your Model](https://jax-ml.github.io/scaling-book/) by Austin, J., Douglas, S., Frostig, R., Levskaya, A., Chen, C., Vikram, S., Lebron, F., Choy, P., Ramasesh, V., Webson, A., & Pope, R. (2025).  

**Credits**: Part of the content in the lectures is based on material from [csci 1390 at Brown](https://cs.brown.edu/courses/csci1390/), created by Deepti Raghavan, and [cs 15-442 at CMU](https://catalyst.cs.cmu.edu/15-884-mlsys-sp21/), by Tianqi Chen.  

**Disclaimer**: This being the first offering of this class, please anticipate technical difficulties. This is not an official course webpage from Plaksha University; this is maintained personally by instructor.

**Feedback**: If you have found the material useful, or have suggestions on how it can be improved, I will be happy to hear from you. Please email me at pankaj.pansari@plaksha.edu.in 

<hr style="border: 3px solid black;">

### Lectures

**Lecture 1: Introduction**

[Slides](https://drive.google.com/file/d/15XROZ6VQ4ZsiYpsDWv1jxMSEGCD6J6x9/view?usp=sharing), [Video lecture](https://youtu.be/TaFBC_U7TOY)

Suggested Reading:

1. Gholami, Amir, et al. "AI and Memory Wall." IEEE Micro Journal ([arxiv link](https://arxiv.org/abs/2403.14123)) 

> This very readable paper presents the interplay between throughput, bandwidth, and end-to-end runtime via a case study on Transformer models.
   
2. Kaplan, Jared, et al. "Scaling laws for neural language models." arXiv:2001.08361 (2020) ([arxiv link](https://arxiv.org/abs/2001.08361))
      
  > An empirical study quantifying the improvement in LLM performance with model size, training dataset size, and amount of compute. Also known as Kaplan's scaling law.

3. Jacob, Austin, et al. [How to Scale Your Model](https://jax-ml.github.io/scaling-book/) (Part 0: Intro)

  > A useful summary of why study of ML Systems is important.

**Lecture 2: Automatic Differentiation**

[Slides](https://drive.google.com/file/d/15f9BvkVyV-6b5sWcqEmzOkrDqZZzW2zZ/view?usp=sharing), [Video lecture](https://youtu.be/4pI13caAZ0E)

Suggested Reading:

1. Kevin Clark, "Computing Neural Network Gradients" ([link](https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf)) 

> A concise refresher on analytical gradient computation for neural networks in terms of matrices and vectors.

2. Roger Grosse, "CSC321 Lecture 10: Automatic Differentiation" ([Slides](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf))

> A presentation of automatic differentiation in the context of the Autograd library. It's nice to see how Autograd builds computation graph in a different manner than PyTorch or Needle. 

3. Chen, Tianqi, et al. "Training Deep Nets with Sublinear Memory Cost" ([arxiv link](https://arxiv.org/abs/1604.06174))

> Seminal paper that introduced the idea of gradient checkpointing.

**Practical 1: Automatic Differentiation Implementation** 

[Video](https://youtu.be/kjHdUqdm_aU)

We do a code walkthrough on how reverse-mode automatic differentiation is implemented in a modern ML framework. We choose [Needle](https://github.com/iwzbi/needle), an educational framework with a similar interface as PyTorch, developed at CMU.

**Lecture 3: Understanding GPU Bottlenecks for ML**

[Slides](https://drive.google.com/file/d/1TRr3Zu9wibIAq7B8lKR0q2QMquexqHFF/view?usp=drive_link), [Video lecture](https://youtu.be/ZFwBQgdE-s4)

Suggested Reading:

1. Stephen Jones, "How GPU Computing Works", GTC 2021 ([Video](https://youtu.be/3l10o0DYJXg?si=XmGT9VRL9HwjY1us))
   
> Excellent introduction to principles behind GPU design and CUDA architecture. Specially good is the discussion of co-design of GPU hardware and CUDA. 

2. Nvidia Docs, "GPU Performance Background User's Guide" ([Link](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html))

> A useful discussion of how different deep learning operations get limited by either compute or memory of GPU. 
   
3. Horace He, "Making Deep Learning Go Brrrr From First Principles" ([Blog post](https://horace.io/brrr_intro.html))

> Nice diagrams and figures to compactly illustrate the 3 components where our ML program spends time - compute, memory, and overhead. 

**Lecture 4: GPU Programming Model**

[Slides](https://drive.google.com/file/d/1FmzTAcrONNt_UIVcOMydtnplx-9r5HpN/view?usp=sharing), [Video lecture](https://youtu.be/oERajeZtiCk)

Suggested Reading:

1. Mark Harris, "An Even Easier Introduction to CUDA" ([Nvidia Technical Blog post](https://developer.nvidia.com/blog/even-easier-introduction-cuda/))
   
 > Introduces GPU execution model by vector addition example. One useful concept used is Unified Memory - a pool of managed memory shared by GPU and CPU, thereby simplifying memory management for the programmer. Also, Mark introduces a new CUDA programming pattern called *grid-stride loop*. 
   
2. Jeremy Howard, "Getting Started With CUDA for Python Programmers"  ([Video](https://youtu.be/nOxKexn3iBo?si=LVgsojMY62Tpr6u9))
   
 > Jeremy shows how to call a CUDA kernel from PyTorch using its `cpp_extension`. He introduces kernels to do RGB to grayscale conversion and matrix multiplication in a more Pythonic way. 
   
3. Sasha Rush, "GPU Puzzles" ([Link](https://github.com/srush/GPU-Puzzles))
  
  > A nice collection of puzzles to implement CUDA kernels in Python. Uses [Numba](https://nvidia.github.io/numba-cuda/index.html) Python JIT Compiler.